model_name: google/flan-t5-base
tokenizer_name: google/flan-t5-base
learning_rate: 5e-3
gradient_checkpointing: false
fp16: false
loss: contrastive
max_length: 400
embedding_size: 768
gradient_accumulation_steps: 16
per_device_train_batch_size: 2
num_train_epochs: 2
warmup_steps: 600
eval_steps: 100
save_steps: 500
datasets:
  - webgpt